{
  "name": "DeepDSL",
  "tagline": "DeepDSL is a domain specific language (DSL) embedded in Scala for writing deep learning network applications.",
  "body": "# DeepDSL\r\n\r\nDeepDSL is a domain specific language (DSL) embedded in Scala for writing deep learning network applications.\r\n\r\n- DeepDSL program compiles into plain Java source program\r\n- The compiled Java source program run on Nvidia GPU by leveraging [JCuda] to interact with the Nvidia CUDA library\r\n\r\n## Performance Benchmark\r\n\r\nBelow we list the runtime and memory performance comparison between DeepDSL, Tensorflow, and Caffe with a single Nvidia Tesla K40c GPU. \r\n\r\n![Runtime performance](https://github.com/deepdsl/deepdsl/raw/master/benchmark/runtime_performance.png)\r\n![Memory performance](https://github.com/deepdsl/deepdsl/raw/master/benchmark/memory_performance.png)\r\n\r\nPlease refer to our paper draft [DeepDSL] for full details. \r\n\r\n## Run DeepDSL compiled programs\r\n- There are several compiled Java source program located at [src/main/java/deepdsl/gen/]. \r\n- These programs train and test on several well-known deep networks: Lenet, Alexnet, Overfeat, Googlenet, Vgg, and ResNet. \r\n\r\n### Maven Prerequisites:\r\n- To build the project using Maven: You just need to install the latest [Apache Maven]\r\n- To run test cases and compiled code for different networks, you also need to: \r\n  - have a [Nvidia CUDA] enabled GPU machine  \r\n  - install 8.X version of [CUDA Toolkit 8.0] and [cuDNN 5] libraries\r\n\r\n### Maven Build (Need to be executed the below in the root folder of the project)\r\n- Windows build: mvn -Pwin64 clean install\r\n- Linux build: mvn -Plinux64 clean install\r\n- OSX build: mvn -Posx64 clean install (This will be for trial purpose only as there're no CUDA based GPU on Mac systems yet)\r\n\r\n### Maven Run (the below uses Alexnet as example, executions with other networks are similar)\r\nAfter the previous Maven Build step, you can cd to the deepdsl-java folder and run the following based on your operating system:\r\n\r\n- Windows build: mvn -Pwin64 exec:java -Dexec.mainClass=\"deepdsl.gen.Alexnet\"\r\n- Linux build: mvn -Plinux64 exec:java -Dexec.mainClass=\"deepdsl.gen.Alexnet\"\r\n- OSX build: mvn -Posx64 exec:java -Dexec.mainClass=\"deepdsl.gen.Alexnet\"\r\n\r\n### IDE notes\r\n- It appears IntelliJ can handle the dependencies correctly once you import the Maven project or simply pull the latest code\r\n- Eclipse, however, after importing Maven project, you may also need to right select deepdsl project -> Maven -> Update Project... -> Ok \r\n  to force refreshing the dependencies, if you have updated from previous build\r\n\r\n## Data handling utils\r\nThere are two util Python scripts under the folder src/main/python (both should be run from the deepdsl project root folder).\r\n\r\n- mnist_data_handler.py: download the mnist data and unzip to the `dataset/mnist` folder\r\n     - to run: `python src/main/python/mnist_data_handler.py`, this will pull and extract the mnist dataset to `dataset/mnist`\r\n- imagenet_data_selector.py: to select given number of images of given number of categories from the original imagenet data\r\n     - to run: `python src/main/python/imagenet_data_selector.py` and then follow the on-screen instructions to apply the desired parameters and run again\r\n        - For example, `python imagenet_data_selector.py ~/data/ILSVRC2012_img_train ~/data/temp 5 50 0.3 0.2`, which selects from 5 categories (50 images per catetory) from ~/data/ILSVRC2012_img_train folder and stores selected images to ~/data/temp folder, where 30% are stored as validation dataset and 20% are stored as test dataset\r\n\r\n### Default location for training and testing data\r\nEach program assumes a location for the training and test data. \r\n\r\n- [Lenet.java] uses Mnist, which is assumed to be located at [dataset/mnist] (please use the script described in the previous section to prepare the dataset).\r\n- Other programs such as [Alexnet.java] use imagenet (as Lmdb database), which is assumed to be located at \"[dataset/imagenet/]ilsvrc12_train_lmdb\" for training data and \"[dataset/imagenet/]ilsvrc12_val_lmdb\" for testing data, where the image sizes are cropped to 224 x 224. Other image sizes should also work since we would randomly cropped the training images to the right size while cropping the testing images at center.\r\n     - Users currently may use tools like [Caffe's imagenet script] `examples/imagenet/create_imagenet.sh` to create the lmdb data from the original Imagenet dataset. Please hang tight, we are adding our scripts soon so you don't have to resort to outside reources.\r\n- For Lmdb data source, users may edit the call to [LmdbFactory].getFactory in the generated Java source to change the max number of training images and test images. The current default is 1000,000 and 10,000 respectively. \r\n- The training and testing all use the same batch size. \r\n\r\n## Adjust learning parameters\r\n- At the start of each file, there are some parameters you can adjust such as learn_rate and moment, as well as training iterations and test iterations\r\n- The batch size for Lenet is set at 500; for Alexnet, Overfeat, and Googlenet is 128; for Vgg and ResNet is set at 64\r\n- At this time, if you want to change batch size, you may want to regenerate the Java source file. Directly editing the Java source might easily miss a few places\r\n\r\n### Default location for trained parameters\r\nEach program will save the trained model, the set of trained parameters (as serialized Java objects), into a default directory. \r\n\r\n- It will try to load saved parameters (if exist) from the same directory when you train the same program again next time\r\n- For example, [Alexnet.java] will try to use the directory \"[src/main/java/deepdsl/gen/]alexnet\" \r\n- You can customize this in the source file directly.\r\n\r\n## Generate Java source\r\nYou can generate Java source for a particular network by running the Scala test program [TestNetwork.scala]. While this is a Scala program, you can run it as a JUnit test to generate Java source code, the generated code will be written to [src/main/java/deepdsl/gen/]. \r\nYou can run this directly from IDE, or cd to deepdsl-java folder and run from command line as the below after modifying the code and executing Maven build.\r\n\r\n- e.g. Overfeat cuda code generation: mvn -Plinux64 test -Dtest=TestNetwork#testOverfeat_cuda (this will generate a new Overfeat.java that overwrites the existing one in [src/main/java/deepdsl/gen/]).\r\n\r\n### Example: generate Lenet\r\n\r\n```scala\r\n    \r\n    val K = 10 // # of classes \r\n    val N = 500; val C = 1; val N1 = 28; val N2 = 28 // batch size, channel, and x/y size\r\n \r\n    // Specifying train dataSet. (code gen will also use this to find test dataSet)\r\n    val y = Vec._new(Mnist, \"label\", \"Y\", N)              \r\n    val x = Vec._new(Mnist, \"image\", \"X\", N, C, N1, N2)  \r\n       \r\n    // followings are tensor functions\r\n    val cv1 = CudaLayer.convolv(\"cv1\", 5, 20)       // convolution layer with kernel 5, stride 1, padding 0, and output channel 20\r\n    val cv2 = CudaLayer.convolv(\"cv2\", 5, 50)\r\n    val mp = CudaLayer.max_pool(2)                  // max pooling with kernel 2 and stride 2\r\n    val flat = Layer.flatten(4, 1)                  // flatten a 4-D tensor to 2-D: axis 0 - 3 becomes axis 0 and  axis 1-3\r\n    val f = Layer.full(\"fc1\", 500)                  // fully connected layer with output dimension 500\r\n    val f2 = Layer.full(\"fc2\", K)                   \r\n    val softmax = CudaLayer.softmax                 \r\n    val relu = CudaLayer.relu(2)                    // ReLU activation function (2-D)\r\n      \r\n    // o is a left-associative function composition operator: f o g o h == (f o g) o h  \r\n    val network = f2 o relu o f o flat o mp o cv2 o mp o cv1 \r\n\r\n    println(typeof(network))                        // typecheck the network and print out the tensor function type\r\n    \r\n    val x1 = x.asCuda                               // load x (images) to GPU memory\r\n    val y1 = y.asIndicator(K).asCuda                // convert y (labels) to indicator vectors and load into GPU memory\r\n    val c = (Layer.log_loss(y1) o softmax o network) (x1) // represent the log-loss of the training data\r\n    val p = (Layer.precision(y1) o network) (x1)    // represent the accuracy of the test data\r\n   \r\n    val param = c.freeVar.toList                    // discover the list of training parameters\r\n    \r\n    // parameters: name, training iterations, test iterations, learn rate, momentum, weight decay, cropping (0 means none)\r\n    val solver = Train(\"lenet\", 100, 10, 0.01f, 0.9f, 0.0005f, 0)\r\n    \r\n    val loop = Loop(c, p, (x, y), param, solver)    // represent the training and testing loop\r\n \r\n    runtimeMemory(loop.train)                       // print out the detailed memory consumption for one training loop\r\n    parameterMemory(loop)                           // print out the parameter memory use\r\n    workspaceMemory(loop.train)                     // print out the GPU (convolution) workspace use (only if you has Nvidia GPU)\r\n    cudnn_gen.print(loop)                           // generate Java source code\r\n\r\n```\r\n\r\n## FAQ\r\n- What should I do if I receive compilation errors in the TestNetwork.scala code after I upgrade code?\r\n     - You can simply delete {your_home_folder}/.m2/repository/deepdsl/deepdsl-compile/0.1/deepdsl-compile-0.1.jar and rebuild.\r\n\r\n[JCuda]: <http://jcuda.org/>\r\n[DeepDSL]: <http://openreview.net/pdf?id=Bks8cPcxe>\r\n\r\n[TestNetwork.scala]: <https://github.com/deepdsl/deepdsl/blob/master/deepdsl-java/src/test/java/deepdsl/TestNetwork.scala>\r\n[LmdbFactory]: <https://github.com/deepdsl/deepdsl/blob/master/deepdsl-java/src/main/java/deepdsl/tensor/LmdbFactory.java>\r\n\r\n[src/main/java/deepdsl/gen/]: <https://github.com/deepdsl/deepdsl/tree/master/deepdsl-java/src/main/java/deepdsl/gen>\r\n\r\n[Lenet.java]: <https://github.com/deepdsl/deepdsl/tree/master/deepdsl-java/src/main/java/deepdsl/gen/Lenet.java>\r\n[Alexnet.java]: <https://github.com/deepdsl/deepdsl/tree/master/deepdsl-java/src/main/java/deepdsl/gen/Alexnet.java>\r\n\r\n[dataset/mnist]: <https://github.com/deepdsl/deepdsl/tree/master/deepdsl-java/dataset/mnist>\r\n[dataset/imagenet/]: <https://github.com/deepdsl/deepdsl/tree/master/deepdsl-java/dataset/imagenet/>\r\n[Caffe's imagenet script]: <https://github.com/BVLC/caffe/tree/master/examples/imagenet>\r\n\r\n[Apache Maven]: <https://maven.apache.org/download.cgi>\r\n\r\n[Nvidia CUDA]: <https://en.wikipedia.org/wiki/CUDA>\r\n\r\n[CUDA Toolkit 8.0]: <https://developer.nvidia.com/cuda-downloads>\r\n[cuDNN 5]: <https://developer.nvidia.com/cudnn>\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}